{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from doom_src import utilities\n",
    "from collections import deque\n",
    "from time import time\n",
    "from vizdoom import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the config file and create a new game instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utilities.get_config('configs/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, actions = utilities.create_environment(config, visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need a DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_D():\n",
    "    \"\"\"\n",
    "    Define the Deep-Q Network to play Doom.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        frame_size = config['frame_size']\n",
    "        stack_size = config['stack_size']\n",
    "        learning_rate = config['learning_rate']\n",
    "        \n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # Convolutional layer 1\n",
    "        model.add(\n",
    "            layers.Convolution2D(\n",
    "                filters=32, \n",
    "                kernel_size=(8, 8),\n",
    "                strides=(4,4),\n",
    "                padding='valid',\n",
    "                input_shape=(frame_size, frame_size, stack_size),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            layers.BatchNormalization(\n",
    "                epsilon = 1e-5,\n",
    "            )\n",
    "        )\n",
    "\n",
    "#         # Convolutional layer 2\n",
    "#         model.add(\n",
    "#             layers.Convolution2D(\n",
    "#                 filters=64, \n",
    "#                 kernel_size=(4, 4),\n",
    "#                 strides=(2,2),\n",
    "#                 padding='valid',\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         model.add(\n",
    "#             layers.BatchNormalization(\n",
    "#                 epsilon = 1e-5,\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         # Convolutional layer 3\n",
    "#         model.add(\n",
    "#             layers.Convolution2D(\n",
    "#                 filters=64, \n",
    "#                 kernel_size=(4, 4),\n",
    "#                 strides=(1,1),\n",
    "#                 padding='valid',\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         model.add(\n",
    "#             layers.BatchNormalization(\n",
    "#                 epsilon = 1e-5,\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "        # Flatten before passing to dense layers\n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        # Dense layer 1\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=128,\n",
    "                activation='relu',\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Dense layer 2\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=3,\n",
    "                activation='relu',\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.op = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='mse', optimizer=self.op)\n",
    "        \n",
    "        tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "        \n",
    "        self.model = model\n",
    "        self.board = tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(policy_network, epsilon, state, actions):\n",
    "    \"\"\"\n",
    "    Implements the epsilon-greedy strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.rand(1) < epsilon:\n",
    "        max_Q = np.random.randint(len(actions))\n",
    "    \n",
    "    else:\n",
    "        # print(state.shape)\n",
    "        q = policy_network.model.predict(state.reshape([1,] + list(state.shape)))\n",
    "        max_Q = np.argmax(q)\n",
    "        # print(q, max_Q)\n",
    "    \n",
    "    action = actions[max_Q]\n",
    "        \n",
    "    return action, max_Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(config, n_episodes, game, actions):\n",
    "    \"\"\"\n",
    "    Train the Q Network\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma           = config['gamma']\n",
    "    skip_rate       = config['skip_rate']\n",
    "    stack_size      = config['stack_size']\n",
    "    frame_size      = config['frame_size']\n",
    "    pretrain_steps  = config['pretrain_steps']\n",
    "    batch_size      = config['batch_size']\n",
    "    memory_size     = config['memory_size']\n",
    "#     annealing_steps = config['annealing_steps']\n",
    "    annealing_steps = int(n_episodes * 0.8)\n",
    "    update_freq = 20\n",
    "    annealing_stop  = config['annealing_stop']\n",
    "    annealing_start = config['annealing_start']\n",
    "    \n",
    "    n_actions = len(actions)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_losses  = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # Initialize the DQN\n",
    "    policy_net = DQN_D(config)\n",
    "    target_net = DQN_D(config)\n",
    "    \n",
    "    # Inititalize the memory buffer\n",
    "    memory = utilities.Memory(memory_size)\n",
    "    \n",
    "    # Initialize the linear annealing scheduler\n",
    "    epsilon = utilities.LinearSchedule(\n",
    "        annealing_steps, \n",
    "        annealing_stop, \n",
    "        annealing_start\n",
    "    )\n",
    "    \n",
    "    # Inititalize the stack of frames\n",
    "    stacked_frames = deque(\n",
    "        [np.zeros((frame_size, frame_size), dtype=np.int) for i in range(stack_size)], \n",
    "        maxlen=stack_size\n",
    "    ) \n",
    "    \n",
    "    # Fill up the memory buffer\n",
    "    utilities.pretrain(\n",
    "        pretrain_steps,\n",
    "        memory,\n",
    "        stack_size,\n",
    "        frame_size,\n",
    "        stacked_frames,\n",
    "        game,\n",
    "        actions\n",
    "    )\n",
    "    print(\"Completed pretraining with memory size:\", len(memory))\n",
    "    \n",
    "#     for episode in tqdm.tqdm(range(n_episodes)):\n",
    "    for episode in range(n_episodes):        \n",
    "        eps = epsilon.value(episode)\n",
    "        episode_reward = 0.0\n",
    "        episode_loss = 0.0\n",
    "        episode_length = 0\n",
    "        \n",
    "        # New episode\n",
    "        game.new_episode()\n",
    "        done = game.is_episode_finished()\n",
    "        \n",
    "        # Initial state\n",
    "        frame = game.get_state().screen_buffer\n",
    "        state = utilities.stack_frames(\n",
    "            stacked_frames, frame, True, stack_size, frame_size\n",
    "        )\n",
    "        \n",
    "        while not done:\n",
    "            # Action selection phase\n",
    "            action, action_index = predict_action(policy_net, eps, state, actions)\n",
    "            print('action:', action_index, action)\n",
    "\n",
    "#             reward = game.make_action(action, skip_rate)\n",
    "            reward = game.make_action(action)\n",
    "            if reward > 0:\n",
    "                print('killed the monster', reward)\n",
    "            print('reward:', reward)\n",
    "\n",
    "            done = game.is_episode_finished()\n",
    "            print('done:', done)\n",
    "\n",
    "            # Next state\n",
    "            if done: # Dead\n",
    "                frame = np.zeros(frame.shape)\n",
    "            \n",
    "            else:\n",
    "                frame = game.get_state().screen_buffer\n",
    "            \n",
    "            next_state = utilities.stack_frames(\n",
    "                stacked_frames, frame, False, stack_size, frame_size\n",
    "            )\n",
    "            \n",
    "            memory.add((state, action_index, reward, next_state, done))\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Learning step\n",
    "            if episode_length % 4 == 0:\n",
    "                batch = np.array(memory.sample(batch_size))\n",
    "\n",
    "                state_b = np.stack(batch[:,0])\n",
    "                action_b = np.stack(batch[:,1])  \n",
    "                reward_b = np.stack(batch[:,2])\n",
    "                next_state_b = np.stack(batch[:,3])\n",
    "                done_b = np.stack(batch[:,4])\n",
    "\n",
    "#                 print('state_b', state_b.shape)\n",
    "#                 print('action_b', action_b.shape)\n",
    "#                 print('action_b0', type(action_b[0]))\n",
    "#                 print('reward_b', reward_b.shape)\n",
    "#                 print('next_state_b', next_state_b.shape)\n",
    "#                 print('done_b', done_b.shape)  \n",
    "\n",
    "#                 print('state_b0', type(state_b[0]))\n",
    "#                 print('action_b0', type(action_b[0]))\n",
    "#                 print('reward_b0', type(reward_b[0]))\n",
    "#                 print('next_state_b0', type(next_state_b[0]))\n",
    "#                 print('done_b0', type(done_b[0]))\n",
    "                \n",
    "                \n",
    "#                 Q_next_b = target_net.model.predict(next_state_b)\n",
    "#                 targets_b = policy_net.model.predict(state_b)\n",
    "                \n",
    "#                 targets_b[range(batch_size), action_b] = reward_b + gamma * np.max(Q_next_b, axis=1) * np.invert(done_b)\n",
    "\n",
    "                target = policy_net.model.predict(state_b)\n",
    "                target_val = policy_net.model.predict(next_state_b)\n",
    "                target_val_ = target_net.model.predict(next_state_b)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    # like Q Learning, get maximum Q value at s'\n",
    "                    # But from target model\n",
    "                    if done_b[i]:\n",
    "                        target[i][action_b[i]] = reward_b[i]\n",
    "                    else:\n",
    "                        # the key point of Double DQN\n",
    "                        # selection of action is from model\n",
    "                        # update is from target model\n",
    "                        a = np.argmax(target_val[i])\n",
    "                        target[i][action_b[i]] = reward_b[i] + gamma * (target_val_[i][a])\n",
    "\n",
    "#                 episode_loss += policy_net.model.train_on_batch(state_b, targets_b)\n",
    "                episode_loss += policy_net.model.train_on_batch(state_b, target)\n",
    "            \n",
    "            if episode_length % update_freq == 0:\n",
    "                target_net.model.set_weights(policy_net.model.get_weights()) \n",
    "\n",
    "            episode_length += 1\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            policy_net.model.save('saved_models/DQN_mk3_policy.h5')\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_losses.append(episode_loss)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    return episode_rewards, episode_losses, policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/.virtualenvs/doom/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pretraining with memory size: 128\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "killed the monster 100.0\n",
      "reward: 100.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: True\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "killed the monster 100.0\n",
      "reward: 100.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: True\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: True\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 2 [0, 0, 1]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -6.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: True\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 1 [0, 1, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: False\n",
      "action: 0 [1, 0, 0]\n",
      "reward: -1.0\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "episode_rewards, episode_losses, policy_net = train_net(config, 5, game, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utilities.plot_reward_curve(episode_rewards, \"DQN - rewards\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utilities.plot_reward_curve(episode_losses, \"DQN - Loss\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
