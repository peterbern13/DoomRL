# DoomRL

This project began as a final project for a reinforcement learning course at Northeastern University. We construct two distinct learning agents for the 1993 game Doom that learn solely from raw visual information in the game. One solver uses deep Q-learning, which was recently employed with great success in solving various games. The other solver uses SARSA, a tabular method for on-policy temporal difference learning that we consider as a representative baseline of the classical reinforcement learning methods. We show that SARSA makes steady progress toward approximating an optimal policy but within a reasonable time frame it only achieves poor average returns, as expected due to the complexity of the problem. The DQN agent requires a GPU to achieve better results.


## Authors

Peter Bernstein and Giorio Severi

